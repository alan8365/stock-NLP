{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "500 Server Error: Internal Server Error for url: https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\Codes\\PycharmProjects\\stock-NLP\\model\\bert-sentiment.ipynb Cell 1'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Codes/PycharmProjects/stock-NLP/model/bert-sentiment.ipynb#ch0000000?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m BertTokenizer, AutoConfig, AutoModelForSequenceClassification\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Codes/PycharmProjects/stock-NLP/model/bert-sentiment.ipynb#ch0000000?line=2'>3</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m BertTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m'\u001b[39;49m\u001b[39mbert-base-cased\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Codes/PycharmProjects/stock-NLP/model/bert-sentiment.ipynb#ch0000000?line=3'>4</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModelForSequenceClassification\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mbert-base-cased-offline\u001b[39m\u001b[39m\"\u001b[39m, num_labels\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stock-nlp\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1763\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/tokenization_utils_base.py?line=1760'>1761</a>\u001b[0m                 resolved_vocab_files[file_id] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/tokenization_utils_base.py?line=1761'>1762</a>\u001b[0m             \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/tokenization_utils_base.py?line=1762'>1763</a>\u001b[0m                 \u001b[39mraise\u001b[39;00m err\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/tokenization_utils_base.py?line=1764'>1765</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(unresolved_files) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/tokenization_utils_base.py?line=1765'>1766</a>\u001b[0m     logger\u001b[39m.\u001b[39minfo(\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/tokenization_utils_base.py?line=1766'>1767</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt load following files from cache: \u001b[39m\u001b[39m{\u001b[39;00munresolved_files\u001b[39m}\u001b[39;00m\u001b[39m and cannot check if these \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/tokenization_utils_base.py?line=1767'>1768</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfiles are necessary for the tokenizer to operate.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/tokenization_utils_base.py?line=1768'>1769</a>\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stock-nlp\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1724\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/tokenization_utils_base.py?line=1721'>1722</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/tokenization_utils_base.py?line=1722'>1723</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/tokenization_utils_base.py?line=1723'>1724</a>\u001b[0m         resolved_vocab_files[file_id] \u001b[39m=\u001b[39m cached_path(\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/tokenization_utils_base.py?line=1724'>1725</a>\u001b[0m             file_path,\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/tokenization_utils_base.py?line=1725'>1726</a>\u001b[0m             cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/tokenization_utils_base.py?line=1726'>1727</a>\u001b[0m             force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/tokenization_utils_base.py?line=1727'>1728</a>\u001b[0m             proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/tokenization_utils_base.py?line=1728'>1729</a>\u001b[0m             resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/tokenization_utils_base.py?line=1729'>1730</a>\u001b[0m             local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/tokenization_utils_base.py?line=1730'>1731</a>\u001b[0m             use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/tokenization_utils_base.py?line=1731'>1732</a>\u001b[0m             user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/tokenization_utils_base.py?line=1732'>1733</a>\u001b[0m         )\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/tokenization_utils_base.py?line=1734'>1735</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m \u001b[39mas\u001b[39;00m error:\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/tokenization_utils_base.py?line=1735'>1736</a>\u001b[0m         \u001b[39mif\u001b[39;00m local_files_only:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stock-nlp\\lib\\site-packages\\transformers\\file_utils.py:1921\u001b[0m, in \u001b[0;36mcached_path\u001b[1;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/file_utils.py?line=1916'>1917</a>\u001b[0m     local_files_only \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/file_utils.py?line=1918'>1919</a>\u001b[0m \u001b[39mif\u001b[39;00m is_remote_url(url_or_filename):\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/file_utils.py?line=1919'>1920</a>\u001b[0m     \u001b[39m# URL, so get it from the cache (downloading if necessary)\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/file_utils.py?line=1920'>1921</a>\u001b[0m     output_path \u001b[39m=\u001b[39m get_from_cache(\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/file_utils.py?line=1921'>1922</a>\u001b[0m         url_or_filename,\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/file_utils.py?line=1922'>1923</a>\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/file_utils.py?line=1923'>1924</a>\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/file_utils.py?line=1924'>1925</a>\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/file_utils.py?line=1925'>1926</a>\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/file_utils.py?line=1926'>1927</a>\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/file_utils.py?line=1927'>1928</a>\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/file_utils.py?line=1928'>1929</a>\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/file_utils.py?line=1929'>1930</a>\u001b[0m     )\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/file_utils.py?line=1930'>1931</a>\u001b[0m \u001b[39melif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(url_or_filename):\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/file_utils.py?line=1931'>1932</a>\u001b[0m     \u001b[39m# File, and it exists.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/file_utils.py?line=1932'>1933</a>\u001b[0m     output_path \u001b[39m=\u001b[39m url_or_filename\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stock-nlp\\lib\\site-packages\\transformers\\file_utils.py:2125\u001b[0m, in \u001b[0;36mget_from_cache\u001b[1;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/file_utils.py?line=2122'>2123</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/file_utils.py?line=2123'>2124</a>\u001b[0m     r \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mhead(url, headers\u001b[39m=\u001b[39mheaders, allow_redirects\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, proxies\u001b[39m=\u001b[39mproxies, timeout\u001b[39m=\u001b[39metag_timeout)\n\u001b[1;32m-> <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/file_utils.py?line=2124'>2125</a>\u001b[0m     _raise_for_status(r)\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/file_utils.py?line=2125'>2126</a>\u001b[0m     etag \u001b[39m=\u001b[39m r\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mX-Linked-Etag\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m r\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mETag\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/file_utils.py?line=2126'>2127</a>\u001b[0m     \u001b[39m# We favor a custom header indicating the etag of the linked resource, and\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/file_utils.py?line=2127'>2128</a>\u001b[0m     \u001b[39m# we fallback to the regular etag header.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/file_utils.py?line=2128'>2129</a>\u001b[0m     \u001b[39m# If we don't have any of those, raise an error.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stock-nlp\\lib\\site-packages\\transformers\\file_utils.py:2052\u001b[0m, in \u001b[0;36m_raise_for_status\u001b[1;34m(request)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/file_utils.py?line=2048'>2049</a>\u001b[0m     \u001b[39melif\u001b[39;00m error_code \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mRevisionNotFound\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/file_utils.py?line=2049'>2050</a>\u001b[0m         \u001b[39mraise\u001b[39;00m RevisionNotFoundError((\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m404 Client Error: Revision Not Found for url: \u001b[39m\u001b[39m{\u001b[39;00mrequest\u001b[39m.\u001b[39murl\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m))\n\u001b[1;32m-> <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/file_utils.py?line=2051'>2052</a>\u001b[0m request\u001b[39m.\u001b[39;49mraise_for_status()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stock-nlp\\lib\\site-packages\\requests\\models.py:960\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/requests/models.py?line=956'>957</a>\u001b[0m     http_error_msg \u001b[39m=\u001b[39m \u001b[39mu\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m Server Error: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m for url: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstatus_code, reason, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39murl)\n\u001b[0;32m    <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/requests/models.py?line=958'>959</a>\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[1;32m--> <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/requests/models.py?line=959'>960</a>\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 500 Server Error: Internal Server Error for url: https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, AutoConfig, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-offline\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from datasets import Dataset\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def data_loading(url):\n",
    "    with open(url, 'r', encoding='utf-8') as f:\n",
    "        # data = json.loads(f.read())\n",
    "        df = pd.read_json(f)\n",
    "        data = df.copy()\n",
    "        # data = df.loc[:, ['sentiment', 'body']]\n",
    "        data = data.loc[df['sentiment'].notnull()]\n",
    "        data['sentiment'] = pd.Categorical(data['sentiment'])\n",
    "        data['label'] = data['sentiment'].cat.codes\n",
    "        data = data.rename(columns={'sentiment': 'labels', 'body': 'sentense'})\n",
    "\n",
    "        return data\n",
    "\n",
    "data_url = '../crawler/stock/data/**.json'\n",
    "url = glob(data_url)[-1]\n",
    "data = data_loading(url)\n",
    "\n",
    "dataset = Dataset.from_pandas(data.loc[:, ['label', 'sentense']])\n",
    "dataset = dataset.remove_columns('__index_level_0__')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(example):\n",
    "    result = tokenizer(example['sentense'], padding='max_length', truncation=True)\n",
    "    \n",
    "    return result\n",
    "\n",
    "encoded_dataset = dataset.map(encode, batched=True)\n",
    "encoded_dataset = encoded_dataset.train_test_split(test_size=0.2)\n",
    "# print(encoded_dataset[0]['sentense'])\n",
    "# print(encoded_dataset[0]['input_ids'])\n",
    "# type(encoded_dataset[0]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=Stocktwit_sentiment_analysis\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_metric\n",
    "\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "%env WANDB_PROJECT=Stocktwit_sentiment_analysis\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"bert-classifier-trainer\",\n",
    "    evaluation_strategy=\"step\",\n",
    "    eval_steps=50,\n",
    "    num_train_epochs=1,\n",
    "    save_steps=1000,\n",
    "    per_device_train_batch_size=4,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"bert-sentiment-classifier-test\"\n",
    ")\n",
    "\n",
    "metric = load_metric(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset['train'],\n",
    "    eval_dataset=encoded_dataset['test'],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./bert-classifier-trainer/test-epoch-1')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "152c7e0fd5b2c01bebd317ef10822ff01094aee6d45d80a66ab90147e1c50abf"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('stock-nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
