{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at C:\\Users\\douli/.cache\\huggingface\\transformers\\a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at C:\\Users\\douli/.cache\\huggingface\\transformers\\a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to C:\\Users\\douli\\.cache\\huggingface\\transformers\\tmprn70t8kg\n",
      "Downloading: 100%|██████████| 426k/426k [00:00<00:00, 436kB/s]\n",
      "storing https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json in cache at C:\\Users\\douli/.cache\\huggingface\\transformers\\226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
      "creating metadata file for C:\\Users\\douli/.cache\\huggingface\\transformers\\226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at C:\\Users\\douli/.cache\\huggingface\\transformers\\6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at C:\\Users\\douli/.cache\\huggingface\\transformers\\226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at C:\\Users\\douli/.cache\\huggingface\\transformers\\ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
      "loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at C:\\Users\\douli/.cache\\huggingface\\transformers\\a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "config = AutoConfig.from_pretrained('bert-base-cased')\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "model = AutoModelForMaskedLM.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(29625, 768)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "from datasets import Dataset\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "def mask_data_loading(url):\n",
    "    def stock_symbol_mask(sentense):\n",
    "        pattern = r'\\$[A-Z]*'\n",
    "        # symbol += re.findall(pattern, sentense)\n",
    "        result = re.sub(pattern, tokenizer.mask_token, sentense)\n",
    "\n",
    "        return result\n",
    "\n",
    "    with open(url, 'r', encoding='utf-8') as f:\n",
    "        df = pd.read_json(f)\n",
    "        data = df.copy()\n",
    "        data = data.loc[df['sentiment'].notnull()]\n",
    "        data['sentiment'] = pd.Categorical(data['sentiment'])\n",
    "        data['sentense'] = data['body'].map(stock_symbol_mask)\n",
    "        data['labels'] = data['body']\n",
    "        symbols = set()\n",
    "        for symbol_list in data['body'].str.findall(r'\\$[A-Z]+'):\n",
    "            for symbol in symbol_list:\n",
    "                symbols.add(symbol)\n",
    "        return data, symbols\n",
    "\n",
    "data_url = '../crawler/stock/data/**.json'\n",
    "url = glob(data_url)[-1]\n",
    "data, symbols = mask_data_loading(url)\n",
    "\n",
    "dataset = Dataset.from_pandas(data.loc[:, ['labels', 'sentense']])\n",
    "dataset = dataset.remove_columns('__index_level_0__')\n",
    "\n",
    "special_tokens_dict = {'additional_special_tokens': list(symbols)}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:19<00:00,  1.29s/ba]\n"
     ]
    }
   ],
   "source": [
    "def encode(example):\n",
    "    label = tokenizer(example['labels'], padding='max_length', truncation=True)\n",
    "    # 101, 51, 1234, 12541, 151\n",
    "    result = tokenizer(example['sentense'], padding='max_length', truncation=True)\n",
    "    # 101, 103, 103, 103\n",
    "    result['label_ids'] = label['input_ids']\n",
    "\n",
    "    # masked_position = [i for i in range(len(result['input_ids'])) if result['input_ids'][i] == tokenizer.mask_token_id]\n",
    "    # result['decoder_input_ids'] = label['input_ids']\n",
    "    # result['labels'] = [-100 for i in label['input_ids']]\n",
    "    # for i in range(len(result['labels'])):\n",
    "    #     if not i in masked_position:\n",
    "    #         result['labels'][i] = -100\n",
    "\n",
    "    return result\n",
    "\n",
    "encoded_dataset = dataset.map(encode, batched=True)\n",
    "\n",
    "# print(encoded_dataset[0]['sentense'])\n",
    "# print(encoded_dataset[0]['input_ids'])\n",
    "# print(encoded_dataset[0]['labels'])\n",
    "# print(encoded_dataset[0]['label_ids'])\n",
    "# print(tokenizer.ids_to_tokens)\n",
    "# print(tokenizer.decode(encoded_dataset[0]['label_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n",
    "\n",
    "# samples = encoded_dataset[:2]\n",
    "# for chunk in data_collator(samples)[\"input_ids\"]:\n",
    "#     print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: sentense. If sentense are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "C:\\Users\\douli\\anaconda3\\envs\\stock-nlp\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4000\n",
      "  Num Epochs = 50\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 50000\n",
      "  1%|          | 500/50000 [02:47<4:34:52,  3.00it/s]Saving model checkpoint to test_trainer\\checkpoint-500\n",
      "Configuration saved in test_trainer\\checkpoint-500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.176, 'learning_rate': 4.9500000000000004e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test_trainer\\checkpoint-500\\pytorch_model.bin\n",
      "  2%|▏         | 1000/50000 [05:37<4:31:07,  3.01it/s]Saving model checkpoint to test_trainer\\checkpoint-1000\n",
      "Configuration saved in test_trainer\\checkpoint-1000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0081, 'learning_rate': 4.9e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test_trainer\\checkpoint-1000\\pytorch_model.bin\n",
      "  3%|▎         | 1500/50000 [08:31<4:26:50,  3.03it/s] Saving model checkpoint to test_trainer\\checkpoint-1500\n",
      "Configuration saved in test_trainer\\checkpoint-1500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0064, 'learning_rate': 4.85e-05, 'epoch': 1.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test_trainer\\checkpoint-1500\\pytorch_model.bin\n",
      "  4%|▍         | 2000/50000 [11:20<4:26:05,  3.01it/s] Saving model checkpoint to test_trainer\\checkpoint-2000\n",
      "Configuration saved in test_trainer\\checkpoint-2000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0071, 'learning_rate': 4.8e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test_trainer\\checkpoint-2000\\pytorch_model.bin\n",
      "  5%|▌         | 2500/50000 [14:11<4:23:50,  3.00it/s] Saving model checkpoint to test_trainer\\checkpoint-2500\n",
      "Configuration saved in test_trainer\\checkpoint-2500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.006, 'learning_rate': 4.75e-05, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test_trainer\\checkpoint-2500\\pytorch_model.bin\n",
      "  6%|▌         | 3000/50000 [17:01<4:19:01,  3.02it/s] Saving model checkpoint to test_trainer\\checkpoint-3000\n",
      "Configuration saved in test_trainer\\checkpoint-3000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0075, 'learning_rate': 4.7e-05, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test_trainer\\checkpoint-3000\\pytorch_model.bin\n",
      "  7%|▋         | 3500/50000 [19:53<4:17:35,  3.01it/s] Saving model checkpoint to test_trainer\\checkpoint-3500\n",
      "Configuration saved in test_trainer\\checkpoint-3500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0053, 'learning_rate': 4.6500000000000005e-05, 'epoch': 3.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test_trainer\\checkpoint-3500\\pytorch_model.bin\n",
      "  8%|▊         | 4000/50000 [22:44<4:13:50,  3.02it/s] Saving model checkpoint to test_trainer\\checkpoint-4000\n",
      "Configuration saved in test_trainer\\checkpoint-4000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.006, 'learning_rate': 4.600000000000001e-05, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test_trainer\\checkpoint-4000\\pytorch_model.bin\n",
      "  9%|▉         | 4500/50000 [25:34<4:11:41,  3.01it/s] Saving model checkpoint to test_trainer\\checkpoint-4500\n",
      "Configuration saved in test_trainer\\checkpoint-4500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0053, 'learning_rate': 4.55e-05, 'epoch': 4.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test_trainer\\checkpoint-4500\\pytorch_model.bin\n",
      " 10%|█         | 5000/50000 [28:26<4:08:25,  3.02it/s] Saving model checkpoint to test_trainer\\checkpoint-5000\n",
      "Configuration saved in test_trainer\\checkpoint-5000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0043, 'learning_rate': 4.5e-05, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test_trainer\\checkpoint-5000\\pytorch_model.bin\n",
      " 11%|█         | 5500/50000 [31:16<4:06:37,  3.01it/s] Saving model checkpoint to test_trainer\\checkpoint-5500\n",
      "Configuration saved in test_trainer\\checkpoint-5500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0038, 'learning_rate': 4.4500000000000004e-05, 'epoch': 5.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test_trainer\\checkpoint-5500\\pytorch_model.bin\n",
      " 12%|█▏        | 6000/50000 [34:07<4:03:21,  3.01it/s] Saving model checkpoint to test_trainer\\checkpoint-6000\n",
      "Configuration saved in test_trainer\\checkpoint-6000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0041, 'learning_rate': 4.4000000000000006e-05, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test_trainer\\checkpoint-6000\\pytorch_model.bin\n",
      " 13%|█▎        | 6500/50000 [36:57<4:00:22,  3.02it/s] Saving model checkpoint to test_trainer\\checkpoint-6500\n",
      "Configuration saved in test_trainer\\checkpoint-6500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0029, 'learning_rate': 4.35e-05, 'epoch': 6.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test_trainer\\checkpoint-6500\\pytorch_model.bin\n",
      " 14%|█▍        | 7000/50000 [39:47<3:57:23,  3.02it/s] Saving model checkpoint to test_trainer\\checkpoint-7000\n",
      "Configuration saved in test_trainer\\checkpoint-7000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0029, 'learning_rate': 4.3e-05, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test_trainer\\checkpoint-7000\\pytorch_model.bin\n",
      " 15%|█▌        | 7500/50000 [42:38<3:55:06,  3.01it/s] Saving model checkpoint to test_trainer\\checkpoint-7500\n",
      "Configuration saved in test_trainer\\checkpoint-7500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0021, 'learning_rate': 4.25e-05, 'epoch': 7.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test_trainer\\checkpoint-7500\\pytorch_model.bin\n",
      " 16%|█▌        | 8000/50000 [45:28<3:53:16,  3.00it/s] Saving model checkpoint to test_trainer\\checkpoint-8000\n",
      "Configuration saved in test_trainer\\checkpoint-8000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0021, 'learning_rate': 4.2e-05, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test_trainer\\checkpoint-8000\\pytorch_model.bin\n",
      " 17%|█▋        | 8500/50000 [48:18<3:49:59,  3.01it/s] Saving model checkpoint to test_trainer\\checkpoint-8500\n",
      "Configuration saved in test_trainer\\checkpoint-8500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0016, 'learning_rate': 4.15e-05, 'epoch': 8.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test_trainer\\checkpoint-8500\\pytorch_model.bin\n",
      " 18%|█▊        | 9000/50000 [51:09<3:46:36,  3.02it/s] Saving model checkpoint to test_trainer\\checkpoint-9000\n",
      "Configuration saved in test_trainer\\checkpoint-9000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0017, 'learning_rate': 4.1e-05, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test_trainer\\checkpoint-9000\\pytorch_model.bin\n",
      " 19%|█▉        | 9500/50000 [53:59<3:44:31,  3.01it/s] Saving model checkpoint to test_trainer\\checkpoint-9500\n",
      "Configuration saved in test_trainer\\checkpoint-9500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0011, 'learning_rate': 4.05e-05, 'epoch': 9.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test_trainer\\checkpoint-9500\\pytorch_model.bin\n",
      " 20%|██        | 10000/50000 [56:50<3:40:48,  3.02it/s]Saving model checkpoint to test_trainer\\checkpoint-10000\n",
      "Configuration saved in test_trainer\\checkpoint-10000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0014, 'learning_rate': 4e-05, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test_trainer\\checkpoint-10000\\pytorch_model.bin\n",
      " 21%|██        | 10500/50000 [59:40<3:37:49,  3.02it/s] Saving model checkpoint to test_trainer\\checkpoint-10500\n",
      "Configuration saved in test_trainer\\checkpoint-10500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0011, 'learning_rate': 3.9500000000000005e-05, 'epoch': 10.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test_trainer\\checkpoint-10500\\pytorch_model.bin\n",
      " 22%|██▏       | 11000/50000 [1:02:30<3:36:30,  3.00it/s]Saving model checkpoint to test_trainer\\checkpoint-11000\n",
      "Configuration saved in test_trainer\\checkpoint-11000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.001, 'learning_rate': 3.9000000000000006e-05, 'epoch': 11.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test_trainer\\checkpoint-11000\\pytorch_model.bin\n",
      " 23%|██▎       | 11500/50000 [1:05:21<3:32:42,  3.02it/s] Saving model checkpoint to test_trainer\\checkpoint-11500\n",
      "Configuration saved in test_trainer\\checkpoint-11500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0009, 'learning_rate': 3.85e-05, 'epoch': 11.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test_trainer\\checkpoint-11500\\pytorch_model.bin\n",
      " 24%|██▍       | 12000/50000 [1:08:12<3:30:24,  3.01it/s] Saving model checkpoint to test_trainer\\checkpoint-12000\n",
      "Configuration saved in test_trainer\\checkpoint-12000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0009, 'learning_rate': 3.8e-05, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test_trainer\\checkpoint-12000\\pytorch_model.bin\n",
      " 25%|██▌       | 12500/50000 [1:11:06<3:27:18,  3.01it/s] Saving model checkpoint to test_trainer\\checkpoint-12500\n",
      "Configuration saved in test_trainer\\checkpoint-12500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0007, 'learning_rate': 3.7500000000000003e-05, 'epoch': 12.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test_trainer\\checkpoint-12500\\pytorch_model.bin\n",
      " 26%|██▌       | 13000/50000 [1:13:57<3:24:59,  3.01it/s] Saving model checkpoint to test_trainer\\checkpoint-13000\n",
      "Configuration saved in test_trainer\\checkpoint-13000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0007, 'learning_rate': 3.7e-05, 'epoch': 13.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test_trainer\\checkpoint-13000\\pytorch_model.bin\n",
      " 27%|██▋       | 13500/50000 [1:16:48<3:22:18,  3.01it/s] Saving model checkpoint to test_trainer\\checkpoint-13500\n",
      "Configuration saved in test_trainer\\checkpoint-13500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0006, 'learning_rate': 3.65e-05, 'epoch': 13.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test_trainer\\checkpoint-13500\\pytorch_model.bin\n",
      " 28%|██▊       | 14000/50000 [1:19:38<3:19:07,  3.01it/s] Saving model checkpoint to test_trainer\\checkpoint-14000\n",
      "Configuration saved in test_trainer\\checkpoint-14000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0007, 'learning_rate': 3.6e-05, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test_trainer\\checkpoint-14000\\pytorch_model.bin\n",
      " 29%|██▉       | 14500/50000 [1:22:28<3:15:53,  3.02it/s] Saving model checkpoint to test_trainer\\checkpoint-14500\n",
      "Configuration saved in test_trainer\\checkpoint-14500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0006, 'learning_rate': 3.55e-05, 'epoch': 14.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test_trainer\\checkpoint-14500\\pytorch_model.bin\n",
      " 30%|███       | 15000/50000 [1:25:19<3:12:56,  3.02it/s] Saving model checkpoint to test_trainer\\checkpoint-15000\n",
      "Configuration saved in test_trainer\\checkpoint-15000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0006, 'learning_rate': 3.5e-05, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test_trainer\\checkpoint-15000\\pytorch_model.bin\n",
      " 31%|███       | 15500/50000 [1:28:09<3:10:40,  3.02it/s] Saving model checkpoint to test_trainer\\checkpoint-15500\n",
      "Configuration saved in test_trainer\\checkpoint-15500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0005, 'learning_rate': 3.45e-05, 'epoch': 15.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test_trainer\\checkpoint-15500\\pytorch_model.bin\n",
      " 32%|███▏      | 16000/50000 [1:31:00<3:07:38,  3.02it/s] Saving model checkpoint to test_trainer\\checkpoint-16000\n",
      "Configuration saved in test_trainer\\checkpoint-16000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0005, 'learning_rate': 3.4000000000000007e-05, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test_trainer\\checkpoint-16000\\pytorch_model.bin\n",
      " 33%|███▎      | 16500/50000 [1:33:50<3:04:53,  3.02it/s] Saving model checkpoint to test_trainer\\checkpoint-16500\n",
      "Configuration saved in test_trainer\\checkpoint-16500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0005, 'learning_rate': 3.35e-05, 'epoch': 16.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test_trainer\\checkpoint-16500\\pytorch_model.bin\n",
      " 34%|███▍      | 17000/50000 [1:36:41<3:02:02,  3.02it/s] Saving model checkpoint to test_trainer\\checkpoint-17000\n",
      "Configuration saved in test_trainer\\checkpoint-17000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0005, 'learning_rate': 3.3e-05, 'epoch': 17.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test_trainer\\checkpoint-17000\\pytorch_model.bin\n",
      " 35%|███▌      | 17500/50000 [1:39:32<2:59:54,  3.01it/s] Saving model checkpoint to test_trainer\\checkpoint-17500\n",
      "Configuration saved in test_trainer\\checkpoint-17500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0004, 'learning_rate': 3.2500000000000004e-05, 'epoch': 17.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test_trainer\\checkpoint-17500\\pytorch_model.bin\n",
      " 36%|███▌      | 18000/50000 [1:42:23<2:56:34,  3.02it/s] Saving model checkpoint to test_trainer\\checkpoint-18000\n",
      "Configuration saved in test_trainer\\checkpoint-18000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0005, 'learning_rate': 3.2000000000000005e-05, 'epoch': 18.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test_trainer\\checkpoint-18000\\pytorch_model.bin\n",
      " 37%|███▋      | 18500/50000 [1:45:15<2:55:34,  2.99it/s] Saving model checkpoint to test_trainer\\checkpoint-18500\n",
      "Configuration saved in test_trainer\\checkpoint-18500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0004, 'learning_rate': 3.15e-05, 'epoch': 18.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test_trainer\\checkpoint-18500\\pytorch_model.bin\n",
      " 38%|███▊      | 19000/50000 [1:48:12<2:52:10,  3.00it/s] Saving model checkpoint to test_trainer\\checkpoint-19000\n",
      "Configuration saved in test_trainer\\checkpoint-19000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0004, 'learning_rate': 3.1e-05, 'epoch': 19.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test_trainer\\checkpoint-19000\\pytorch_model.bin\n",
      " 39%|███▉      | 19500/50000 [1:51:04<2:48:06,  3.02it/s] Saving model checkpoint to test_trainer\\checkpoint-19500\n",
      "Configuration saved in test_trainer\\checkpoint-19500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0004, 'learning_rate': 3.05e-05, 'epoch': 19.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test_trainer\\checkpoint-19500\\pytorch_model.bin\n",
      " 40%|████      | 20000/50000 [1:53:57<2:45:42,  3.02it/s] Saving model checkpoint to test_trainer\\checkpoint-20000\n",
      "Configuration saved in test_trainer\\checkpoint-20000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0003, 'learning_rate': 3e-05, 'epoch': 20.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test_trainer\\checkpoint-20000\\pytorch_model.bin\n",
      " 41%|████      | 20500/50000 [1:56:49<2:43:25,  3.01it/s] Saving model checkpoint to test_trainer\\checkpoint-20500\n",
      "Configuration saved in test_trainer\\checkpoint-20500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0003, 'learning_rate': 2.95e-05, 'epoch': 20.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test_trainer\\checkpoint-20500\\pytorch_model.bin\n",
      " 42%|████▏     | 21000/50000 [1:59:41<2:39:56,  3.02it/s] Saving model checkpoint to test_trainer\\checkpoint-21000\n",
      "Configuration saved in test_trainer\\checkpoint-21000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0003, 'learning_rate': 2.9e-05, 'epoch': 21.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test_trainer\\checkpoint-21000\\pytorch_model.bin\n",
      " 43%|████▎     | 21500/50000 [2:02:36<2:38:03,  3.01it/s] Saving model checkpoint to test_trainer\\checkpoint-21500\n",
      "Configuration saved in test_trainer\\checkpoint-21500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0003, 'learning_rate': 2.8499999999999998e-05, 'epoch': 21.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test_trainer\\checkpoint-21500\\pytorch_model.bin\n",
      " 44%|████▍     | 22000/50000 [2:05:29<2:34:26,  3.02it/s] Saving model checkpoint to test_trainer\\checkpoint-22000\n",
      "Configuration saved in test_trainer\\checkpoint-22000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0003, 'learning_rate': 2.8000000000000003e-05, 'epoch': 22.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test_trainer\\checkpoint-22000\\pytorch_model.bin\n",
      " 45%|████▌     | 22500/50000 [2:08:23<2:31:27,  3.03it/s] Saving model checkpoint to test_trainer\\checkpoint-22500\n",
      "Configuration saved in test_trainer\\checkpoint-22500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0002, 'learning_rate': 2.7500000000000004e-05, 'epoch': 22.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test_trainer\\checkpoint-22500\\pytorch_model.bin\n",
      " 46%|████▌     | 23000/50000 [2:11:17<2:29:34,  3.01it/s] Saving model checkpoint to test_trainer\\checkpoint-23000\n",
      "Configuration saved in test_trainer\\checkpoint-23000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0004, 'learning_rate': 2.7000000000000002e-05, 'epoch': 23.0}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at C:\\cb\\pytorch_1000000000000\\work\\caffe2\\serialize\\inline_container.cc:300] . unexpected pos 317232064 vs 317231952",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\stock-nlp\\lib\\site-packages\\torch\\serialization.py:380\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/torch/serialization.py?line=378'>379</a>\u001b[0m \u001b[39mwith\u001b[39;00m _open_zipfile_writer(opened_file) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m--> <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/torch/serialization.py?line=379'>380</a>\u001b[0m     _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[0;32m    <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/torch/serialization.py?line=380'>381</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stock-nlp\\lib\\site-packages\\torch\\serialization.py:604\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/torch/serialization.py?line=602'>603</a>\u001b[0m num_bytes \u001b[39m=\u001b[39m storage\u001b[39m.\u001b[39mnbytes()\n\u001b[1;32m--> <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/torch/serialization.py?line=603'>604</a>\u001b[0m zip_file\u001b[39m.\u001b[39;49mwrite_record(name, storage\u001b[39m.\u001b[39;49mdata_ptr(), num_bytes)\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 28] No space left on device",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32me:\\Codes\\PycharmProjects\\stock-NLP\\model\\retrain.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Codes/PycharmProjects/stock-NLP/model/retrain.ipynb#ch0000006?line=21'>22</a>\u001b[0m train_datset \u001b[39m=\u001b[39m encoded_dataset\u001b[39m.\u001b[39mshuffle()\u001b[39m.\u001b[39mselect(\u001b[39mrange\u001b[39m(\u001b[39m4000\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Codes/PycharmProjects/stock-NLP/model/retrain.ipynb#ch0000006?line=23'>24</a>\u001b[0m trainer \u001b[39m=\u001b[39m Seq2SeqTrainer(\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Codes/PycharmProjects/stock-NLP/model/retrain.ipynb#ch0000006?line=24'>25</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Codes/PycharmProjects/stock-NLP/model/retrain.ipynb#ch0000006?line=25'>26</a>\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Codes/PycharmProjects/stock-NLP/model/retrain.ipynb#ch0000006?line=26'>27</a>\u001b[0m     train_dataset\u001b[39m=\u001b[39mtrain_datset,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Codes/PycharmProjects/stock-NLP/model/retrain.ipynb#ch0000006?line=27'>28</a>\u001b[0m     compute_metrics\u001b[39m=\u001b[39mcompute_metrics,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Codes/PycharmProjects/stock-NLP/model/retrain.ipynb#ch0000006?line=28'>29</a>\u001b[0m )\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Codes/PycharmProjects/stock-NLP/model/retrain.ipynb#ch0000006?line=30'>31</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stock-nlp\\lib\\site-packages\\transformers\\trainer.py:1475\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/trainer.py?line=1471'>1472</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mepoch \u001b[39m=\u001b[39m epoch \u001b[39m+\u001b[39m (step \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m/\u001b[39m steps_in_epoch\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/trainer.py?line=1472'>1473</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_end(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m-> <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/trainer.py?line=1474'>1475</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/trainer.py?line=1475'>1476</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/trainer.py?line=1476'>1477</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_substep_end(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stock-nlp\\lib\\site-packages\\transformers\\trainer.py:1606\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/trainer.py?line=1602'>1603</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_report_to_hp_search(trial, epoch, metrics)\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/trainer.py?line=1604'>1605</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol\u001b[39m.\u001b[39mshould_save:\n\u001b[1;32m-> <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/trainer.py?line=1605'>1606</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save_checkpoint(model, trial, metrics\u001b[39m=\u001b[39;49mmetrics)\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/trainer.py?line=1606'>1607</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_save(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stock-nlp\\lib\\site-packages\\transformers\\trainer.py:1678\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[1;34m(self, model, trial, metrics)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/trainer.py?line=1674'>1675</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstore_flos()\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/trainer.py?line=1676'>1677</a>\u001b[0m output_dir \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(run_dir, checkpoint_folder)\n\u001b[1;32m-> <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/trainer.py?line=1677'>1678</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave_model(output_dir, _internal_call\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/trainer.py?line=1678'>1679</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeepspeed:\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/trainer.py?line=1679'>1680</a>\u001b[0m     \u001b[39m# under zero3 model file itself doesn't get saved since it's bogus! Unless deepspeed\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/trainer.py?line=1680'>1681</a>\u001b[0m     \u001b[39m# config `stage3_gather_fp16_weights_on_model_save` is True\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/trainer.py?line=1681'>1682</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeepspeed\u001b[39m.\u001b[39msave_checkpoint(output_dir)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stock-nlp\\lib\\site-packages\\transformers\\trainer.py:2101\u001b[0m, in \u001b[0;36mTrainer.save_model\u001b[1;34m(self, output_dir, _internal_call)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/trainer.py?line=2097'>2098</a>\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeepspeed\u001b[39m.\u001b[39msave_checkpoint(output_dir)\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/trainer.py?line=2099'>2100</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mshould_save:\n\u001b[1;32m-> <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/trainer.py?line=2100'>2101</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save(output_dir)\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/trainer.py?line=2102'>2103</a>\u001b[0m \u001b[39m# Push to the Hub when `save_model` is called by the user.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/trainer.py?line=2103'>2104</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpush_to_hub \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m _internal_call:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stock-nlp\\lib\\site-packages\\transformers\\trainer.py:2153\u001b[0m, in \u001b[0;36mTrainer._save\u001b[1;34m(self, output_dir, state_dict)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/trainer.py?line=2150'>2151</a>\u001b[0m         torch\u001b[39m.\u001b[39msave(state_dict, os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(output_dir, WEIGHTS_NAME))\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/trainer.py?line=2151'>2152</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/trainer.py?line=2152'>2153</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49msave_pretrained(output_dir, state_dict\u001b[39m=\u001b[39;49mstate_dict)\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/trainer.py?line=2153'>2154</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/trainer.py?line=2154'>2155</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39msave_pretrained(output_dir)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stock-nlp\\lib\\site-packages\\transformers\\modeling_utils.py:1082\u001b[0m, in \u001b[0;36mPreTrainedModel.save_pretrained\u001b[1;34m(self, save_directory, save_config, state_dict, save_function, push_to_hub, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/modeling_utils.py?line=1079'>1080</a>\u001b[0m \u001b[39m# If we save using the predefined names, we can load using `from_pretrained`\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/modeling_utils.py?line=1080'>1081</a>\u001b[0m output_model_file \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(save_directory, WEIGHTS_NAME)\n\u001b[1;32m-> <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/modeling_utils.py?line=1081'>1082</a>\u001b[0m save_function(state_dict, output_model_file)\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/modeling_utils.py?line=1083'>1084</a>\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel weights saved in \u001b[39m\u001b[39m{\u001b[39;00moutput_model_file\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/transformers/modeling_utils.py?line=1085'>1086</a>\u001b[0m \u001b[39mif\u001b[39;00m push_to_hub:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stock-nlp\\lib\\site-packages\\torch\\serialization.py:381\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/torch/serialization.py?line=378'>379</a>\u001b[0m     \u001b[39mwith\u001b[39;00m _open_zipfile_writer(opened_file) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/torch/serialization.py?line=379'>380</a>\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[1;32m--> <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/torch/serialization.py?line=380'>381</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/torch/serialization.py?line=381'>382</a>\u001b[0m _legacy_save(obj, opened_file, pickle_module, pickle_protocol)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\stock-nlp\\lib\\site-packages\\torch\\serialization.py:260\u001b[0m, in \u001b[0;36m_open_zipfile_writer_buffer.__exit__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/torch/serialization.py?line=258'>259</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__exit__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/torch/serialization.py?line=259'>260</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfile_like\u001b[39m.\u001b[39;49mwrite_end_of_file()\n\u001b[0;32m    <a href='file:///c%3A/Users/douli/anaconda3/envs/stock-nlp/lib/site-packages/torch/serialization.py?line=260'>261</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer\u001b[39m.\u001b[39mflush()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\cb\\pytorch_1000000000000\\work\\caffe2\\serialize\\inline_container.cc:300] . unexpected pos 317232064 vs 317231952"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"test_trainer\",\n",
    "#     per_device_train_batch_size=2,\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=encoded_dataset,\n",
    "#     compute_metrics=compute_metrics,\n",
    "# )\n",
    "\n",
    "# Seq2SeqTrainer\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"test_trainer\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=50\n",
    ")\n",
    "\n",
    "train_datset = encoded_dataset.shuffle().select(range(4000))\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_datset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9999986886978149,\n",
       "  'token': 29351,\n",
       "  'token_str': '$ T S L A',\n",
       "  'sequence': 'to the sky!!!'},\n",
       " {'score': 2.3993442255232367e-07,\n",
       "  'token': 29039,\n",
       "  'token_str': '$ I M P P',\n",
       "  'sequence': 'to the sky!!!'},\n",
       " {'score': 1.1034241254037624e-07,\n",
       "  'token': 29448,\n",
       "  'token_str': '$ M S F T',\n",
       "  'sequence': 'to the sky!!!'},\n",
       " {'score': 7.492760545346755e-08,\n",
       "  'token': 29010,\n",
       "  'token_str': '$ P L T R',\n",
       "  'sequence': 'to the sky!!!'},\n",
       " {'score': 7.289293790790907e-08,\n",
       "  'token': 29250,\n",
       "  'token_str': '$ S P Y',\n",
       "  'sequence': 'to the sky!!!'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0\n",
    ")\n",
    "\n",
    "s = '[MASK] to the sky!!!'\n",
    "fill_mask(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_summary())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "152c7e0fd5b2c01bebd317ef10822ff01094aee6d45d80a66ab90147e1c50abf"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('stock-nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
